{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Parsing HTML\n",
    "from lxml import html\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "#get html and tree\n",
    "html_page_link = 'webpage.html'\n",
    "tree = html.parse(html_page_link).getroot()\n",
    "\n",
    "painting_records = {\n",
    "    'artist_name': None,\n",
    "    'painting_name': None,\n",
    "    'price_gbp': None,\n",
    "    'price_us': None,\n",
    "    'price_gbp_est': None,\n",
    "    'price_us_est': None,\n",
    "    'image_link': None,\n",
    "    'sell_date': None\n",
    "}\n",
    "\n",
    "# Parse the artist name\n",
    "# Newline are removed before assignent\n",
    "for element in tree.iter('h1'):\n",
    "    painting_records['artist_name'] = element.text_content().split('\\r\\n')[0]\n",
    "\n",
    "# Parse painting name\n",
    "for element in tree.iter('h2'):\n",
    "    painting_records['painting_name'] = element.text_content().split('\\r\\n')[0]\n",
    "\n",
    "# All prices should be parsed into integer\n",
    "# Parse price GBP\n",
    "painting_price_element_gbp = tree.get_element_by_id('main_center_0_lblPriceRealizedPrimary')\n",
    "painting_price_content_gbp = painting_price_element_gbp.text_content()\n",
    "painting_records['price_gbp'] = int(painting_price_content_gbp.split(' ')[1].replace(',', ''))\n",
    "\n",
    "# Parse price US\n",
    "painting_price_element_us = tree.get_element_by_id('main_center_0_lblPriceRealizedSecondary')\n",
    "painting_price_content_us = painting_price_element_us.text_content()\n",
    "painting_records['price_us'] = int(painting_price_content_us.split(' ')[1].replace(',', ''))\n",
    "\n",
    "# Parse price GBP est\n",
    "painting_price_element_gbp_est = tree.get_element_by_id('main_center_0_lblPriceEstimatedPrimary')\n",
    "painting_price_content_gbp_est = painting_price_element_gbp_est.text_content()\n",
    "painting_records['price_gbp_est'] = int(painting_price_content_gbp_est.split(' ')[1].replace(',', ''))\n",
    "\n",
    "# Parse price US est\n",
    "painting_price_element_us_est = tree.get_element_by_id('main_center_0_lblPriceEstimatedSecondary')\n",
    "painting_price_content_us_est = painting_price_element_us_est.text_content()\n",
    "painting_records['price_us_est'] = int(painting_price_content_us_est.split(' ')[1].replace(',', ''))\n",
    "\n",
    "# Parse the image link\n",
    "image_element = tree.get_element_by_id('imgLotImage')\n",
    "painting_records['image_link'] = image_element.attrib['src']\n",
    "\n",
    "# Parse the sell date\n",
    "sell_date_element = tree.get_element_by_id('main_center_0_lblSaleDate')\n",
    "sell_date_string = sell_date_element.text_content().split(',')[0]\n",
    "painting_records['sell_date'] = datetime.strptime(sell_date_string, '%d %B %Y').date()\n",
    "\n",
    "# Convert dict to pd dataframe\n",
    "painting_records_dataframe = pd.DataFrame([painting_records])\n",
    "painting_records_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rawDim</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19×52cm</td>\n",
       "      <td>19.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50 x 66,4 cm</td>\n",
       "      <td>50.0</td>\n",
       "      <td>66.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168.9 x 274.3 x 3.8 cm (66 1/2 x 108 x 1 1/2 in.)</td>\n",
       "      <td>168.9</td>\n",
       "      <td>274.3</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sheet: 16 1/4 × 12 1/4 in. (41.3 × 31.1 cm) Im...</td>\n",
       "      <td>35.6</td>\n",
       "      <td>25.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5 by 5in</td>\n",
       "      <td>12.7</td>\n",
       "      <td>12.7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              rawDim  height  width  depth\n",
       "0                                            19×52cm    19.0   52.0    NaN\n",
       "1                                       50 x 66,4 cm    50.0   66.4    NaN\n",
       "2  168.9 x 274.3 x 3.8 cm (66 1/2 x 108 x 1 1/2 in.)   168.9  274.3    3.8\n",
       "3  Sheet: 16 1/4 × 12 1/4 in. (41.3 × 31.1 cm) Im...    35.6   25.1    NaN\n",
       "4                                           5 by 5in    12.7   12.7    NaN"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Regex\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "dim_df = pd.read_csv(\"dim_df_correct.csv\")\n",
    "dim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Height: 19.0 Width: 52.0\n",
      "Height: 50.0 Width: 66.4\n",
      "Height: 168.9 Width: 274.3 Depth: 3.8\n",
      "Height: 16.0 Width: 1.0 Depth: 4.0\n",
      "Height: 5.0 Width: 12.7\n"
     ]
    }
   ],
   "source": [
    "########### reg expressions\n",
    "\n",
    "# Preprocessing for special character ','\n",
    "def replaceSpecialChar(dimension_string):\n",
    "    dimension_string_seperated = dimension_string.split('x')\n",
    "    for i, item in enumerate(dimension_string_seperated):\n",
    "        if (',' in item) is True:\n",
    "            for char in item:\n",
    "                item = item.replace(',',\".\")\n",
    "                dimension_string_seperated[i] = item\n",
    "    return ' '.join(dimension_string_seperated)\n",
    "\n",
    "#Preprocessing for inches\n",
    "def convertInchesToCm(dimension_string):\n",
    "    dimension_string_seperated = dimension_string.split('by')\n",
    "    if len(dimension_string_seperated) > 1:\n",
    "        for i, item in enumerate(dimension_string_seperated):\n",
    "            if 'in' in item:\n",
    "                number = re.findall('-?\\d+\\.?\\d*',item)\n",
    "                item = int(number[0]) * 2.54 # convert from inches to cm\n",
    "                dimension_string_seperated[i] = str(item)\n",
    "    return ' '.join(dimension_string_seperated)\n",
    "\n",
    "#Preprocessing\n",
    "dim_df['rawDim'] = dim_df['rawDim'].map( lambda x : replaceSpecialChar(x))\n",
    "dim_df['rawDim'] = dim_df['rawDim'].map( lambda x : convertInchesToCm(x))\n",
    "\n",
    "for dimString in dim_df['rawDim']:    \n",
    "    # If any other content contains ',' then switch to a .\n",
    "    for i, item in enumerate(dimString.split('x')):\n",
    "        if (',' in item) is True:\n",
    "            for char in item:\n",
    "                item = item.replace(',',\".\")\n",
    "                \n",
    "    numbers = re.findall('-?\\d+\\.?\\d*',dimString)\n",
    "    try:\n",
    "        print('Height: {} Width: {} Depth: {}'.format(np.float64(numbers[0]), np.float64(numbers[1]), np.float64(numbers[2])))\n",
    "    except IndexError as e:\n",
    "        if len(numbers) < 3: \n",
    "            print('Height: {} Width: {}'.format(np.float64(numbers[0]), np.float64(numbers[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Web crawler\n",
    "listings_link = 'https://www.bearspace.co.uk/purchase/'\n",
    "res = requests.get(listings_link)\n",
    "listings_page_content = html.fromstring(res.content)\n",
    "gallery_records = {\n",
    "    'url': [],\n",
    "    'title': [],\n",
    "    'media': [0] *20 ,\n",
    "    'height': [],\n",
    "    'width': [],\n",
    "    'price_gbp': []\n",
    "}\n",
    "\n",
    "for element in listings_page_content.iter('a'):\n",
    "    for item in element.items():\n",
    "        if 'class' in item:\n",
    "            if item[1] == '_2zTHN _2AHc6':\n",
    "                # Obtain URL from the 'a' tags\n",
    "                url = element.attrib['href']\n",
    "                gallery_records['url'].append(url)\n",
    "                \n",
    "                # Scrape the product page\n",
    "                product_page_content = html.fromstring(requests.get(url).content)\n",
    "                \n",
    "                # Obtain the price of the gallery\n",
    "                for product_page_div in product_page_content.iter('div'):\n",
    "                    for item in product_page_div.items():\n",
    "                        if 'class' in item:\n",
    "                            if item[1] == '_2sFaY':\n",
    "                                for i, product_page_span in enumerate(product_page_div.iter('span')):\n",
    "                                    if i == 0:\n",
    "                                        gallery_records['price_gbp'].append(product_page_span.text_content())\n",
    "                                    continue\n",
    "                # Obtain title form the 'h3' tags\n",
    "                for child_element in element.iter('h3'):\n",
    "                    gallery_records['title'].append(child_element.text_content())\n",
    "\n",
    "# For each URL scrape the product page and obtain the media, height and width\n",
    "for i, url in enumerate(gallery_records['url']):\n",
    "    # Scrape the product page\n",
    "    product_page_content = html.fromstring(requests.get(url).content)\n",
    "    \n",
    "    for pd_element in product_page_content.iter('pre'):\n",
    "        for pd_element_description in pd_element.iter('p'):\n",
    "\n",
    "            # Obtain the dimensions and media from the first 'p' tag\n",
    "            numbers = re.findall('-?\\d+\\.?\\d*', pd_element_description.text_content())\n",
    "\n",
    "            # If there are no dimension then p tag contains the media\n",
    "            if len(numbers) == 2:\n",
    "                gallery_records['height'].append(numbers[0])\n",
    "                gallery_records['width'].append(numbers[1])\n",
    "            # Description of painting will not contain numbers and spaces    \n",
    "            elif len(numbers) == 0 and pd_element_description.text_content().isspace() is False:\n",
    "                gallery_records['media'][i] = pd_element_description.text_content()\n",
    "\n",
    "\n",
    "gallery_records_dataframe = pd.DataFrame(gallery_records)\n",
    "gallery_records_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Data\n",
    "# Task 1 : Describe inner join, left join, right join, full join.7\n",
    "print('Inner Join')\n",
    "print('This is the combination of rows in two tables by using a  coloumns existing in two seperate tables')\n",
    "print('Inner join combines both table based on the condition that there is a matching coloumn from both table, the resulting table will contain rows that are only present in both tables')\n",
    "print('\\n')\n",
    "\n",
    "print('Left Join')\n",
    "print('This works by combining two tables bassed on a condition that matches the two tables, also rows that are not matched from the table placed on the left of the clause will be present but will have null values if no matching row in the second table is found')\n",
    "print('Left join takes all content from the table placed on the left regardsless of if the condiiton is met, and also includes content from both tables that match the condition')\n",
    "print('\\n')\n",
    "\n",
    "print('Right Join')\n",
    "print('This works by combining two tables bassed on a condition that matches the two tables, also rows that are not matched from the table placed on the right will be present but will have null values if no matching row in the second table is found')\n",
    "print('The right join combined the two tables based on matching conditiona and also includes the rows from the table placed on the right of the clause, even though there are no matches')\n",
    "print('\\n')\n",
    "\n",
    "print('Full Join')\n",
    "print('This is a combination of both right join and left join and the resulting table will return all rows that are matched or unmatched based on condition from both sides of the query statement')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandasql as ps\n",
    "\n",
    "flights = pd.read_csv(\"flights.csv\")\n",
    "airports = pd.read_csv(\"airports.csv\")\n",
    "weather = pd.read_csv(\"weather.csv\")\n",
    "airlines = pd.read_csv(\"airlines.csv\")\n",
    "\n",
    "# Add full airline name to the flights dataframe and show the arr_time, origin, dest and the name of the airline.\n",
    "query1 = \"\"\"SELECT flights.arr_time, flights.origin, flights.dest, flights.flight, flights.distance, airlines.name FROM airlines LEFT JOIN flights ON airlines.carrier = flights.carrier\"\"\"\n",
    "# save query1 as dataframe\n",
    "new_data_frame = ps.sqldf(query1, locals())\n",
    "\n",
    "# Filter resulting data.frame to include only flights containing the word JetBlue\n",
    "query2 = \"\"\"SELECT * FROM new_data_frame WHERE name LIKE '%JetBlue%'\"\"\"\n",
    "# save query 1 as dataframe\n",
    "new_data_frame = ps.sqldf(query2, locals())\n",
    "\n",
    "# Summarise the total number of flights by origin in ascending.\n",
    "query3 = \"\"\"SELECT origin, sum(flight) as numFlight FROM new_data_frame GROUP BY origin ORDER BY origin ASC\"\"\"\n",
    "# save query 3 as dataframe\n",
    "new_data_frame = ps.sqldf(query3, locals())\n",
    "print(new_data_frame)\n",
    "print(\"Might be missing the planes csv file\")\n",
    "# Filter resulting data.frame to return only origins with more than 10,000 flights.F\n",
    "query4 = \"\"\"SELECT * FROM new_data_frame WHERE flights > 10000\"\"\"\n",
    "# save query 4 as dataframe\n",
    "# # print query4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
